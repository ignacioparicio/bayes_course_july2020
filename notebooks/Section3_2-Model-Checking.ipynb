{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Checking\n",
    "\n",
    "After running an MCMC simulation, `sample` returns an object (as of PyMC 3.9, an `InferenceData` object) containing the samples for all the stochastic and deterministic random variables. The final step in Bayesian computation is model checking, in order to ensure that inferences derived from your sample are valid. \n",
    "\n",
    "There are **two components** to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect **lack of convergence** in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. \n",
    "\n",
    "The second component of model checking, goodness of fit, is used to check the **internal validity** of the model, by comparing predictions from the model to the data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "There are a handful of easy-to-use methods for checking convergence. Since you cannot prove convergence, but only show lack of convergence, there is no single method that is foolproof. So, its best to look at a suite of diagnostics together. \n",
    "\n",
    "We will cover the canonical set of checks:\n",
    "\n",
    "- Traceplot\n",
    "- Divergences\n",
    "- R-hat\n",
    "- Effective Sample Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceplot \n",
    "\n",
    "This is a simple plot that is a good quick check to make sure nothing is obviously wrong, and is usually the first diagnostic step you will take. You've seen these already: just the time series of samples for an individual variable.\n",
    "\n",
    "Let's run the PKU model again as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_data = pd.read_csv('../data/pku_data.csv')\n",
    "\n",
    "unique_papers = set(pku_data['Paper ID'])\n",
    "paper_map = {p:i for i,p in enumerate(unique_papers)}\n",
    "paper_id = pku_data['Paper ID'].replace(paper_map)\n",
    "phe_std = ((pku_data.Phe - pku_data.Phe.mean()) / pku_data.Phe.std()).values\n",
    "iq = pku_data['IQ'].values\n",
    "concurrent_measurement = pku_data['Concurrent'].values\n",
    "\n",
    "with pm.Model() as pku_model:\n",
    "\n",
    "    μ_int = pm.Normal('μ_int', mu=100, sigma=1e3)\n",
    "    σ_int = pm.HalfCauchy('σ_int', beta=5)\n",
    "    β_0 = pm.Normal('β_0', μ_int, sigma=σ_int, shape=len(unique_papers))\n",
    "    β_1 = pm.Normal('β_1', mu=0, sigma=1e3)\n",
    "    β_2 = pm.Normal('β_2', mu=0, sigma=1e3)\n",
    "\n",
    "    μ_iq = β_0[paper_id] + β_1*phe_std + β_2*concurrent_measurement\n",
    "\n",
    "    σ_iq = pm.HalfCauchy('σ_iq', beta=1)\n",
    "    iq_like = pm.Normal('iq_like', mu=μ_iq, sigma=σ_iq, observed=iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "\n",
    "    pku_trace = pm.sample(500, tune=0, step=pm.Metropolis(), return_inferencedata=True, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_trace` function from ArViZ by default generates a kernel density plot and a trace plot, with a different color for each chain of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(pku_trace, var_names=['μ_int', 'σ_iq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample is deliberately inadequate. Looking at the trace plot, the problems should be apparent.\n",
    "\n",
    "Can you identify the issues, based on what you learned in the previous section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Take a quiz!\n",
    "\n",
    "[See how well you can identify sampling problems by looking at their traceplots](https://canyon289.github.io/bayesian-model-evaluation/lessonplans/mcmc_basics/#/14)\n",
    "\n",
    "The slides will show you a trace, and you have to guess whether the sampling is from one of:\n",
    "\n",
    "- MCMC with step size too small\n",
    "- MCMC with step size too large\n",
    "- MCMC with adequate step size\n",
    "- Independent samples from distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergences\n",
    "\n",
    "As we have seen, Hamiltonian Monte Carlo (and NUTS) performs numerical integration in order to explore the posterior distribution of a model. When the integration goes wrong, it can go dramatically wrong. \n",
    "\n",
    "For example, here are some Hamiltonian trajectories on the distribution of two correlated variables. Can you spot the divergent path?\n",
    "\n",
    "![divering HMC](images/diverging_hmc.png)\n",
    "\n",
    "The reason that this happens is that there may be parts of the posterior which are **hard to explore** for geometric reasons. Two ways of solving divergences are\n",
    "\n",
    "1. **Set a higher \"target accept\" rate**: Similarly (but not the same) as for Metropolis-Hastings, larger integrator steps lead to lower acceptance rates. A higher `target_accept` will generally cause a smaller step size, and more accurate integration.\n",
    "2. **Reparametrize**: If you can write your model in a different way that has the same joint probability density, you might do that. A lot of work is being done to automate this, since it requires careful work, and one goal of a probabilistic programming language is to iterate quickly. See [Hoffmann, Johnson, Tran (2018)](https://arxiv.org/abs/1811.11926), [Gorinova, Moore, Hoffmann (2019)](https://arxiv.org/abs/1906.03028), and there is work on this also in [symbolic pymc](https://github.com/pymc-devs/symbolic-pymc).\n",
    "\n",
    "You should be wary of a trace that contains many divergences (particularly those clustered in particular regions of the parameter space), and give thought to how to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergence example\n",
    "\n",
    "The trajectories above are from a famous example of a difficult geometry: Neal's funnel. It is problematic because the geometry is very different in some regions of the state space relative to others. Specifically, for hierarchical models, as the scale parameter changes in size so do the values of the parameters it is constraining. When the variance is close to zero, the parameter space is very constrained relative to the majority of the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neals_funnel(dims=1):\n",
    "    with pm.Model() as funnel:\n",
    "        v = pm.Normal('v', 0, 3)\n",
    "        x_vec = pm.MvNormal('x_vec', mu=tt.zeros(dims), cov=2 * tt.exp(v) * tt.eye(dims), shape=dims)\n",
    "    return funnel\n",
    "\n",
    "with neals_funnel():\n",
    "    funnel_trace = pm.sample(random_seed=RANDOM_SEED, return_inferencedata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3 provides us feedback on divergences, including a count and a recommendation on how to address them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverging_ind = funnel_trace['diverging'].nonzero()[0]\n",
    "diverging_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = pm.plot_joint(trace)\n",
    "ax.plot(trace['v'][diverging_ind], trace['x_vec'][diverging_ind], 'y.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of this using the radon example from the first section. Specifically, we will run the random-slopes model, which has a hierarchical model for the basement effect or radon measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import radon data\n",
    "radon_data = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "\n",
    "counties = radon_data.county.unique()\n",
    "n_counties = counties.shape[0]\n",
    "county = radon_data.county_code.values\n",
    "log_radon = radon_data.log_radon.values\n",
    "floor_measure = radon_data.floor.values\n",
    "log_uranium = np.log(radon_data.Uppm.values)\n",
    "county_lookup = dict(zip(counties, np.arange(n_counties)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope:\n",
    "    \n",
    "    # Priors\n",
    "    μ_b = pm.Normal('μ_b', mu=0., sigma=10)\n",
    "    σ_b = pm.HalfCauchy('σ_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    # Random slopes\n",
    "    b = pm.Normal('b', mu=μ_b, sigma=σ_b, shape=n_counties)\n",
    "    \n",
    "    # Model error\n",
    "    σ_y = pm.HalfCauchy('σ_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=σ_y, observed=log_radon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope:\n",
    "    varying_slope_trace = pm.sample(2000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine the traces of the slope variance and any one of the county slopes, we can see a pathology when the group variance gets very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(varying_slope_trace.get_values('σ_b', chains=0), alpha=.5);\n",
    "axs[0].set(ylabel='σ_b');\n",
    "axs[1].plot(varying_slope_trace.get_values('b', chains=0), alpha=.05);\n",
    "axs[1].set(ylabel='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when the chain reaches the lower end of the parameter space for $\\sigma_b$, it appears to get \"stuck\" and the entire sampler, including the random slopes `b`, mixes poorly. \n",
    "\n",
    "Jointly plotting the random effect variance and one of the individual random slopes demonstrates what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(varying_slope_trace['b'][:, 10], name='slope')\n",
    "y = pd.Series(varying_slope_trace['σ_b'], name='slope group variance')\n",
    "diverging = varying_slope_trace['diverging']\n",
    "\n",
    "jp = sns.jointplot(x, y, ylim=(0, .7), stat_func=None, alpha=0.3)\n",
    "jp.ax_joint.plot(x[diverging], y[diverging], 'yo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the group variance is small, this implies that the individual random slopes are themselves close to the group mean. In itself, this is not a problem, since this is the behavior we expect. However, if the sampler is tuned for the wider (unconstrained) part of the parameter space, it has trouble in the areas of higher curvature. The consequence of this is that the neighborhood close to the lower bound of $\\sigma_b$ is sampled poorly; indeed, in our chain it is not sampled at all below 0.1. The result of this will be biased inference.\n",
    "\n",
    "The `plot_parallel` function in the ArViZ library is a convenient way to identify patterns in divergent traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_parallel(varying_slope_trace, var_names=['b'], figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've spotted the problem, what can we do about it? The best way to deal with this issue is to reparameterize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Non-centered Parameterization\n",
    "\n",
    "The partial pooling model specified above uses a **centered** parameterization of the slope random effect. That is, the individual county effects are distributed around a county mean, with a spread controlled by the hierarchical standard deviation parameter. \n",
    "\n",
    "Here is the DAG of this centered model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the issue with sampling geometry by **reparameterizing** our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope_noncentered:\n",
    "    \n",
    "    # Priors\n",
    "    μ_b = pm.Normal('μ_b', mu=0., sigma=10)\n",
    "    σ_b = pm.HalfCauchy('σ_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    \n",
    "    # Non-centered random slopes\n",
    "    # Centered: b = Normal('b', μ_b, sigma=σ_b, shape=counties)\n",
    "    υ = pm.Normal('υ', mu=0, sigma=1, shape=n_counties)\n",
    "    b = pm.Deterministic(\"b\", μ_b + υ * σ_b)\n",
    "    \n",
    "    # Model error\n",
    "    σ_y =pm.HalfCauchy('σ_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=σ_y, observed=log_radon)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **non-centered** parameterization. By this, we mean that the random deviates are no longer explicitly modeled as being centered on $\\mu_b$. Instead, they are independent standard normals $\\upsilon$, which are then scaled by the appropriate value of $\\sigma_b$, before being location-transformed by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope_noncentered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model samples much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope_noncentered:\n",
    "    noncentered_trace = pm.sample(2000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bottlenecks in the traces are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(noncentered_trace.get_values('σ_b', chains=0), alpha=.5);\n",
    "axs[0].set(ylabel='σ_b');\n",
    "axs[1].plot(noncentered_trace.get_values('b', chains=0), alpha=.5);\n",
    "axs[1].set(ylabel='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we are now fully exploring the support of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(noncentered_trace['b'][:, 75], name='slope')\n",
    "y = pd.Series(noncentered_trace['σ_b'], name='slope group variance')\n",
    "\n",
    "sns.jointplot(x, y, ylim=(0, .7), stat_func=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "pm.plot_posterior(varying_slope_trace, var_names=['σ_b'], ax=ax1, color='LightSeaGreen')\n",
    "pm.plot_posterior(noncentered_trace, var_names=['σ_b'], ax=ax2, color='LightSeaGreen')\n",
    "ax1.set_title('Centered (top) and non-centered (bottom)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *Gelman-Rubin statistic*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "varaince (B) and within-chain varaince (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. \n",
    "\n",
    "In ArViZ, the `summary` table, or a `plot_forest` with the `r_hat` flag set, will calculate $\\hat{R}$ for each stochastic node in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Clearly the model above has not yet converged (we only ran it for 100 iterations without tuning, after all). Try running the `pku_model` for a larger number of iterations, and see when $\\hat{R}$ converges to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Sample Size\n",
    "\n",
    "In general, samples drawn from MCMC algorithms will be autocorrelated. Unless the autocorrelation is very severe, this is not a big deal, other than the fact that autocorrelated chains may require longer sampling in order to adequately characterize posterior quantities of interest. The calculation of autocorrelation is performed for each lag $i=1,2,\\ldots,k$ (the correlation at lag 0 is, of course, 1) by: \n",
    "\n",
    "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n",
    "\n",
    "where $\\hat{\\text{Var}}(\\theta | y)$ is the same estimated variance as calculated for the Gelman-Rubin statistic, and $V_i$ is the variogram at lag $i$ for $\\theta$:\n",
    "\n",
    "$$\\text{V}_i = \\frac{1}{m(n-i)}\\sum_{j=1}^m \\sum_{k=i+1}^n (\\theta_{jk} - \\theta_{j(k-i)})^2$$\n",
    "\n",
    "This autocorrelation can be visualized using the `plot_autocorr` function in ArViZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(pku_trace, var_names=['σ_iq', 'μ_int'], combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see very severe autocorrelation in `μ_int`, which is not surprising given the trace that we observed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of correlation in an MCMC sample influences the **effective sample size** (ESS) of the sample. The ESS estimates how many *independent* draws contain the same amount of information as the *dependent* sample obtained by MCMC sampling.\n",
    "\n",
    "Given a series of samples $x_j$, the empirical mean is\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{j=1}^n x_j\n",
    "$$\n",
    "\n",
    "and the variance of the estimate of the empirical mean is \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n},\n",
    "$$\n",
    "where $\\sigma^2$ is the true variance of the underlying distribution.\n",
    "\n",
    "Then the effective sample size is defined as the denominator that makes this relationship still be true:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n_{\\text{eff}}}.\n",
    "$$\n",
    "\n",
    "The effective sample size is estimated using the partial sum:\n",
    "\n",
    "$$\\hat{n}_{eff} = \\frac{mn}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
    "\n",
    "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative.\n",
    "\n",
    "The issue here is related to the fact that we are **estimating** the effective sample size from the fit output. Values of $n_{eff} / n_{iter} < 0.001$ indicate a biased estimator, resulting in an overestimate of the true effective sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehtari *et al* (2019) recommend an ESS of at least 400 to ensure reliable estimates of variances and autocorrelations. They also suggest running at least 4 chains before calculating any diagnostics.\n",
    "\n",
    "Its important to note that ESS can vary across the quantiles of the MCMC chain being sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(pku_trace, var_names=['μ_int']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ArViZ, we can visualize the evolution of ESS as the MCMC sample accumulates. When the model is converging properly, both lines in this plot should be approximately linear.\n",
    "\n",
    "The standard ESS estimate, which mainly assesses how well the centre of the distribution is resolved, is referred to as **bulk-ESS**. In order to estimate intervals reliably, it is also important to consider the **tail-ESS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(pku_trace, var_names=['μ_int'], kind='evolution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESS statistics can also be tabulated, by generating a `summary` of the parameters of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to want to **thin** the chain to eliminate the autocorrelation (*e.g.* taking every 20th sample from traces with autocorrelation as high as 20), but this is a waste of time. Since thinning deliberately throws out the majority of the samples, no efficiency is gained; you ultimately require more samples to achive a particular desired sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFMI calculation is only available in samples that were simulated using HMC or NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(pku_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "\n",
    "    pku_trace = pm.sample(return_inferencedata=True, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the overall distribution of \n",
    "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(pku_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "As noted at the beginning of this section, convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model to converge, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC3. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model. Consider the PKU model, \n",
    "where IQ is modeled as a Gaussian random variable, which is thought to be influenced by blood Phe levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution of IQ uses the same functional form as the data likelihood, in this case a normal stochastic. Here is the corresponding sample from the posterior predictive distribution (we typically need very few samples relative to the MCMC sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "    pku_ppc = pm.sample_posterior_predictive(pku_trace.posterior, samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree to which simulated data correspond to observations can be evaluated visually. This allows for a qualitative comparison of model-based replicates and observations. If there is poor fit, the true value of the data may appear in the tails of the histogram of replicated data, while a good fit will tend to show the true data in high-probability regions of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 4, figsize=(12,15), sharex=True, sharey=True)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    i = np.random.randint(0, pku_ppc['iq_like'].shape[1])\n",
    "    ax.hist(pku_ppc['iq_like'][:, i], alpha=0.3)\n",
    "    ax.vlines(iq[i], 0, 100)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantitative approach is to calculate quantiles of each observed data point relative to the corresponding distribution of posterior-simulated values. For an adequate fit, there should not be severe peaks in the histogram near zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "plt.hist([np.round(percentileofscore(x, y)/100, 2) for x,y in zip(pku_ppc['iq_like'], iq)], bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457–472.\n",
    "\n",
    "[Vehtari, Gelman, Simpson, Carpenter, Bürkner (2019)](https://arxiv.org/abs/1903.08008) Rank-normalization, folding, and localization: An improved $\\hat{R}$ for assessing convergence of MCMC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('bayes_course': conda)",
   "language": "python",
   "name": "python37664bitbayescoursecondaac15308fe835468386ae397edcb53ec9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
