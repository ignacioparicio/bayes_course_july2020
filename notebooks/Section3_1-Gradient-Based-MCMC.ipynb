{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based Markov Chain Monte Carlo\n",
    "\n",
    "Having introduced first-generation MCMC methods previously, we will now turn our attention to a more sophisticated class of algorithm, which improve upon random-walk jump algorithms by using information about the posterior distribution to inform candidate transitions: namely, gradient information.\n",
    "\n",
    "While flexible and easy to implement, Metropolis-Hastings sampling is a random walk\n",
    "sampler that might not be statistically efficient for many models. Specifically, for models of high dimension, random walk jumping algorithms do not perform well. It is not enough to simply guess at the next sample location; we need to make each iteration a useful draw from the posterior whenever we can, in order to have an efficient sampler for bigger models.\n",
    "\n",
    "![from Hoffman and Gelman 2014](http://d.pr/i/RAA+)\n",
    "\n",
    "Since Bayesian inference is all about calculating expectations over posteriors, what we seek is an algorithm that explores the area of the parameter space that contains most of the non-zero probability. This region is called the **typical set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What's a Typical Set?\n",
    "\n",
    "The typical set is where most of the probability density (mass) lies in a particular volume associated with the distribution. As the dimension of a model increases, this set moves progressively further from the mode, and becomes more singular, as the result of concentration of measure.\n",
    "\n",
    "![mode volume](images/mode_volume.png)\n",
    "\n",
    "(from *Betancourt 2018*)\n",
    "\n",
    "\n",
    "The typical set is a product of both the density, which is highest at the mode, and volume (that we integrate over), which increasingly becomes larger away from the mode as dimensionality increases. In fact, at high dimensions, the region around the mode contributes almost nothing to the expectation. We need an algorithm that will find this narrow region and explore it efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Dynamics\n",
    "\n",
    "In this context, and when sampling from continuous variables, Hamiltonian (or Hybrid) Monte Carlo (HMC) is a powerful tool. It avoids\n",
    "random walk behavior by simulating a physical system governed by **Hamiltonian dynamics**, potentially avoiding tricky conditional\n",
    "distributions in the process.\n",
    "\n",
    "![skate park](images/skate_park.png)\n",
    "\n",
    "In HMC, model samples are obtained by simulating a physical system, using differential equations, where particles move about a high-dimensional landscape, subject to **potential and kinetic energies**. \n",
    "\n",
    "![Hamiltonian dynamics](images/hamiltonian_dynamics.png)\n",
    "\n",
    "(from *Carroll 2019*)\n",
    "\n",
    "In the simulations above, momentum is represented by the size and direction of the arrows. Larger (smaller) arrows indicate large kinetic (potential) energy. These trajectories are **energy-conserving**.\n",
    "\n",
    "Adapting the notation from [Neal (1993)](http://www.cs.toronto.edu/~radford/review.abstract.html), particles are characterized by a position vector or state $s \\in \\mathcal{R}^D$ and velocity vector $\\phi \\in \\mathcal{R}^D$.\n",
    "\n",
    "The joint **canonical distribution** of the position and velocity can be expressed as a product of the marginal position (which is of interest) and the conditional distribution of the velocity (a nuisance parameter):\n",
    "\n",
    "$$\\pi(s, \\phi) = \\pi(\\phi | s) \\pi(s)$$\n",
    "\n",
    "This joint probability can also be written in terms of an invariant **Hamiltonian function**:\n",
    "\n",
    "$$\\pi(s, \\phi) \\propto \\exp(-H(s,\\phi))$$\n",
    "\n",
    "The Hamiltonian is then defined as the sum of potential energy $E(s)$ and kinetic energy\n",
    "$K(\\phi)$, as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(s,\\phi) = E(s) + K(\\phi)= E(s) + \\frac{1}{2} \\sum_i \\phi_i^2\n",
    "$$\n",
    "\n",
    "Instead of sampling $p(s)$ directly, HMC operates by sampling from the canonical distribution.\n",
    "\n",
    "$$p(s,\\phi) = \\frac{1}{Z} \\exp(-\\mathcal{H}(s,\\phi))=p(s)p(\\phi)$$.\n",
    "\n",
    "If we choose a momentum that is independent of position, marginalizing over $\\phi$ is trivial and recovers the original distribution of interest.\n",
    "\n",
    "The Hamiltonian $\\mathcal{H}$ is independent of the parameterization of the model, and therefore, captures the geometry of the phase space distribution, including typical set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State $s$ and velocity $\\phi$ are modified such that\n",
    "$\\mathcal{H}(s,\\phi)$ remains constant throughout the simulation. The\n",
    "differential equations are given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{ds_i}{dt} &= \\frac{\\partial \\mathcal{H}}{\\partial \\phi_i} = \\phi_i \\\\\n",
    "\\frac{d\\phi_i}{dt} &= - \\frac{\\partial \\mathcal{H}}{\\partial s_i}\n",
    "= - \\frac{\\partial E}{\\partial s_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As shown in [Neal (1993)](http://www.cs.toronto.edu/~radford/review.abstract.html), \n",
    "the above transformation preserves volume and is\n",
    "**reversible**. \n",
    "\n",
    "> Reversibility is the key property of MCMC algorithms, and is established when the **detailed balance** equation is satisfied:\n",
    "> $$\\pi(x) P(x|y) = \\pi(y) P(y|x)$$\n",
    "\n",
    "The above dynamics can thus be used as transition operators\n",
    "of a Markov chain and will leave $p(s,\\phi)$ invariant. That chain by\n",
    "itself is not ergodic however, since simulating the dynamics maintains a\n",
    "fixed Hamiltonian $\\mathcal{H}(s,\\phi)$. HMC thus alternates Hamiltonian\n",
    "dynamic steps, with sampling of the velocity. Because $p(s)$ and\n",
    "$p(\\phi)$ are independent, sampling $\\phi_{new} \\sim p(\\phi|s)$ is\n",
    "trivial since $p(\\phi|s)=p(\\phi)$, where $p(\\phi)$ is often taken to be\n",
    "the univariate Gaussian.\n",
    "\n",
    "Other distributions can be chosen for momentum, but a Gaussian allows for the easy calculation of gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Hamiltonian Dynamics\n",
    "\n",
    "In practice, we cannot simulate Hamiltonian dynamics exactly because of the problem of time discretization: its not possible to simulate in continuous time, so we alternate between updating position $s$ and velocity $\\phi$. To maintain invariance of the Markov chain however, care must be taken to preserve the properties of *volume conservation* and *time reversibility*. The **leap-frog algorithm** maintains these properties and operates in 3 steps:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi_i(t + \\epsilon/2) &= \\phi_i(t) - \\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t)) \\\\\n",
    "s_i(t + \\epsilon) &= s_i(t) + \\epsilon \\phi_i(t + \\epsilon/2) \\\\\n",
    "\\phi_i(t + \\epsilon) &= \\phi_i(t + \\epsilon/2) - \\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t + \\epsilon)) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We thus perform a half-step update of the velocity at time\n",
    "$t+\\epsilon/2$, which is then used to compute $s(t + \\epsilon)$ and\n",
    "$\\phi(t + \\epsilon)$.\n",
    "\n",
    "To perform $n$ leapfrog steps, we first need to define a function over\n",
    "which the algorithm can iterate. Instead of implementing leap frog verbatim, notice that we can obtain\n",
    "$s(t + n \\epsilon)$ and $\\phi(t + n \\epsilon)$ by performing an initial\n",
    "half-step update for $\\phi$, followed by $n$ full-step updates for\n",
    "$s,\\phi$ and one last half-step update for $\\phi$. In loop form, this\n",
    "gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\phi_i(t + \\epsilon/2) &= \\phi_i(t) -\n",
    "\\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t)) \\\\\n",
    "s_i(t + \\epsilon) &= s_i(t) + \\epsilon \\phi_i(t + \\epsilon/2) \\\\\n",
    "\\text{For } m \\in [2,n]\\text{, perform full updates: } \\\\\n",
    "\\qquad\n",
    "\\phi_i(t + (m - 1/2)\\epsilon) &= \\phi_i(t + (m-3/2)\\epsilon) -\n",
    "\\epsilon \\frac{\\partial{}}{\\partial s_i} E(s(t + (m-1)\\epsilon)) \\\\\n",
    "\\qquad\n",
    "s_i(t + m\\epsilon) &= s_i(t) + \\epsilon \\phi_i(t + (m-1/2)\\epsilon) \\\\\n",
    "\\phi_i(t + n\\epsilon) &= \\phi_i(t + (n-1/2)\\epsilon) -\n",
    "\\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t + n\\epsilon)) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The inner-loop defined above is implemented by the following\n",
    "`leapfrog` function, with `pos`, `vel` and `step` replacing\n",
    "$s,\\phi$ and $\\epsilon$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import theano\n",
    "from theano import shared, function\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dynamics(initial_pos, initial_vel, stepsize, n_steps, energy_fn):\n",
    "    \n",
    "\n",
    "    def leapfrog(pos, vel, step):\n",
    "\n",
    "        # Gradient calculation\n",
    "        dE_dpos = tt.grad(energy_fn(pos).sum(), pos)\n",
    "        \n",
    "        new_vel = vel - step * dE_dpos\n",
    "        new_pos = pos + step * new_vel\n",
    "        return [new_pos, new_vel], {}\n",
    "\n",
    "    # An initial half-step in velocity\n",
    "    initial_energy = energy_fn(initial_pos)\n",
    "    dE_dpos = tt.grad(initial_energy.sum(), initial_pos)\n",
    "    vel_half_step = initial_vel - 0.5 * stepsize * dE_dpos\n",
    "\n",
    "    # ... followed by one full position step\n",
    "    pos_full_step = initial_pos + stepsize * vel_half_step\n",
    "\n",
    "    # Perform full velocity-step updates afterwards, using scan\n",
    "    (all_pos, all_vel), scan_updates = theano.scan(leapfrog,\n",
    "            outputs_info=[\n",
    "                dict(initial=pos_full_step),\n",
    "                dict(initial=vel_half_step),\n",
    "                ],\n",
    "            non_sequences=[stepsize],\n",
    "            n_steps=n_steps - 1)\n",
    "    \n",
    "    # Our final position after integrating\n",
    "    final_pos = all_pos[-1]\n",
    "    final_vel = all_vel[-1]\n",
    "    \n",
    "    # One final half-step in velocity to complete the algorithm\n",
    "    energy = energy_fn(final_pos)\n",
    "    final_vel = final_vel - 0.5 * stepsize * tt.grad(energy.sum(), final_pos)\n",
    "\n",
    "    # return new proposal state\n",
    "    return final_pos, final_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the position update occurs along all dimensions simultaneously, and not just one parameter at a time. This is a noted improvement over the Metropolis-class algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of the dynamics function imply a number of decisions that need to be made by the user: How many steps do we want to take? How large should each step be? How much momentum should be provided?\n",
    "\n",
    "We justified our choice of momentum previously, a random univariate Gaussian. The decisions regarding the number and size of steps has implications for the **path length** used to obtain a sample. Here are three different path lengths and the resulting samples (blue points):\n",
    "\n",
    "![HMC samples](images/HMC_samples.png)\n",
    "\n",
    "(from *Carroll 2019*)\n",
    "\n",
    "As you might expect, different step sizes and numbers are appropriate for different models, depending on the associated geometry, and therefore tuning is usually necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept / Reject\n",
    "\n",
    "In practice, using finite stepsizes $\\epsilon$ will not preserve $\\mathcal{H}(s,\\phi)$ exactly and will introduce bias in the simulation.\n",
    "Also, rounding errors due to the use of floating point numbers means that the above transformation will not be perfectly reversible.\n",
    "\n",
    "HMC cancels these effects **exactly** by adding a Metropolis accept/reject stage, after $n$ leapfrog steps. The new state\n",
    "$\\chi' = (s',\\phi')$ is accepted with probability $p_{acc}(\\chi,\\chi')$, defined as:\n",
    "\n",
    "$$p_{acc}(\\chi,\\chi') = min \\left( 1, \\frac{\\exp(-\\mathcal{H}(s',\\phi')}{\\exp(-\\mathcal{H}(s,\\phi)} \\right)$$\n",
    "\n",
    "### HMC Algorithm\n",
    "\n",
    "Given all of these components, we can now express the HMC algorithm.\n",
    "\n",
    "Though there is some complex math involved, the algorithm itself is conceptually simple. It involves three steps:\n",
    "\n",
    "1.  sample a new velocity from a univariate Gaussian distribution\n",
    "2.  perform $n$ leapfrog steps to obtain the new state $\\chi'$\n",
    "3.  perform accept/reject move of $\\chi'$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hmc_move` function below implements a full HMC step. Given a matrix of initial states\n",
    "$s \\in \\mathcal{R}^{N \\times D}$ (`positions`) and energy function\n",
    "$E(s)$ (`energy_fn`), it defines the symbolic graph for computing\n",
    "`n_steps` of HMC, using a given `stepsize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc_move(s_rng, positions, energy_fn, stepsize, n_steps):\n",
    "\n",
    "    # sample random velocity using independent normals\n",
    "    initial_vel = s_rng.normal(size=positions.shape)\n",
    "\n",
    "    # perform simulation of particles subject to Hamiltonian dynamics\n",
    "    final_pos, final_vel = simulate_dynamics(\n",
    "            initial_pos=positions,\n",
    "            initial_vel=initial_vel,\n",
    "            stepsize=stepsize,\n",
    "            n_steps=n_steps,\n",
    "            energy_fn=energy_fn)\n",
    "\n",
    "    # accept/reject the proposed move based on the joint distribution\n",
    "    accept = metropolis_hastings_accept(\n",
    "            energy_prev=hamiltonian(positions, initial_vel, energy_fn),\n",
    "            energy_next=hamiltonian(final_pos, final_vel, energy_fn),\n",
    "            s_rng=s_rng)\n",
    "\n",
    "    return accept, final_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `metropolis_hastings_accept` and `hamiltonian` are helper\n",
    "functions, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings_accept(energy_prev, energy_next, s_rng):\n",
    "    \n",
    "    ediff = energy_prev - energy_next\n",
    "    return (tt.exp(ediff) - s_rng.uniform(size=energy_prev.shape)) >= 0\n",
    "\n",
    "\n",
    "def kinetic_energy(vel):\n",
    "    \n",
    "    return 0.5 * (vel ** 2).sum(axis=1)\n",
    "\n",
    "\n",
    "def hamiltonian(pos, vel, energy_fn):\n",
    "    \"\"\"\n",
    "    Returns the Hamiltonian (sum of potential and kinetic energy) for the given\n",
    "    velocity and position. Assumes mass is 1.\n",
    "    \"\"\"\n",
    "    return energy_fn(pos) + kinetic_energy(vel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's next? As with Metropolis samplers, we want to be able to **tune** our step size so that it performs integration adequately. The following function implements adaptation by tracking the average acceptance rate of the HMC move proposals across\n",
    "many simulations, using an exponential moving average with time constant `1 - avg_acceptance_slowness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc_updates(positions, stepsize, avg_acceptance_rate, final_pos, accept,\n",
    "                 target_acceptance_rate, stepsize_inc, stepsize_dec,\n",
    "                 stepsize_min, stepsize_max, avg_acceptance_slowness):\n",
    "\n",
    "    \"\"\"\n",
    "    POSITION UPDATES\n",
    "    \n",
    "    Uses results of the acceptance test to update position\n",
    "    \"\"\"\n",
    "\n",
    "    accept_matrix = accept.dimshuffle(0, *(('x',) * (final_pos.ndim - 1)))\n",
    "    # if accept is True, update to `final_pos` else stay put\n",
    "    new_positions = tt.switch(accept_matrix, final_pos, positions)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    STEP SIZE UPDATES\n",
    "    \n",
    "    If the acceptance rate is two low, reduce the stepsize; if it is too high, \n",
    "    increase the stepsize\n",
    "    \"\"\"\n",
    "    \n",
    "    _new_stepsize = tt.switch(avg_acceptance_rate > target_acceptance_rate,\n",
    "                              stepsize * stepsize_inc, stepsize * stepsize_dec)\n",
    "    # maintain stepsize in [stepsize_min, stepsize_max]\n",
    "    new_stepsize = tt.clip(_new_stepsize, stepsize_min, stepsize_max)\n",
    "\n",
    "    # Update acceptance rate with exponential moving average\n",
    "    mean_dtype = theano.scalar.upcast(accept.dtype, avg_acceptance_rate.dtype)\n",
    "    new_acceptance_rate = tt.add(\n",
    "            avg_acceptance_slowness * avg_acceptance_rate,\n",
    "            (1.0 - avg_acceptance_slowness) * accept.mean(dtype=mean_dtype))\n",
    "\n",
    "    return [(positions, new_positions),\n",
    "            (stepsize, new_stepsize),\n",
    "            (avg_acceptance_rate, new_acceptance_rate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many interlocking pieces comprising the HMC sampler, its convenient to wrap everything up in a Python class. The constructor creates the symbolic graph for performing an HMC simulation (using `hmc_move` and `hmc_updates`). The graph is then compiled into the `simulate` function, a theano function which runs the simulation and updates the required shared variables. \n",
    "\n",
    "The sampler is called via the `draw` method, which calls the Theano function `simulate` and returns a copy of the contents of the shared variable `self.positions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharedX(X, name):\n",
    "    return shared(np.asarray(X, dtype=theano.config.floatX), name=name)\n",
    "\n",
    "class HMC(object):\n",
    "\n",
    "    def __init__(self, shared_positions, energy_fn,\n",
    "            initial_stepsize=0.01, target_acceptance_rate=.9, n_steps=20,\n",
    "            stepsize_dec=0.98,\n",
    "            stepsize_min=0.001,\n",
    "            stepsize_max=0.25,\n",
    "            stepsize_inc=1.02,\n",
    "            avg_acceptance_slowness=0.9,\n",
    "            seed=12345):\n",
    "        \n",
    "        self.positions = shared_positions\n",
    "        \n",
    "        batchsize = shared_positions.shape[0]\n",
    "\n",
    "        # allocate shared variables\n",
    "        self.stepsize = stepsize = sharedX(initial_stepsize, 'hmc_stepsize')\n",
    "        self.avg_acceptance_rate = avg_acceptance_rate = sharedX(target_acceptance_rate,\n",
    "                                      'avg_acceptance_rate')\n",
    "        \n",
    "        s_rng = tt.shared_randomstreams.RandomStreams(seed)\n",
    "\n",
    "        # define graph for an `n_steps` HMC simulation\n",
    "        accept, final_pos = hmc_move(\n",
    "                s_rng,\n",
    "                shared_positions,\n",
    "                energy_fn,\n",
    "                stepsize,\n",
    "                n_steps)\n",
    "\n",
    "        # define the dictionary of updates, to apply on every `simulate` call\n",
    "        simulate_updates = hmc_updates(\n",
    "                shared_positions,\n",
    "                stepsize,\n",
    "                avg_acceptance_rate,\n",
    "                final_pos=final_pos,\n",
    "                accept=accept,\n",
    "                stepsize_min=stepsize_min,\n",
    "                stepsize_max=stepsize_max,\n",
    "                stepsize_inc=stepsize_inc,\n",
    "                stepsize_dec=stepsize_dec,\n",
    "                target_acceptance_rate=target_acceptance_rate,\n",
    "                avg_acceptance_slowness=avg_acceptance_slowness)\n",
    "\n",
    "        # compile theano function\n",
    "        self.simulate = function([], [], updates=simulate_updates)\n",
    "\n",
    "\n",
    "    def draw(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a new position obtained after `n_steps` of HMC simulation.\n",
    "        \"\"\"\n",
    "        self.simulate()\n",
    "        return self.positions.get_value(borrow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Sampler\n",
    "\n",
    "Let's test our implementation of HMC by sampling from a multivariate normal distribution of length 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation hyperparameters\n",
    "burnin = 2000\n",
    "n_samples = 1000\n",
    "dim = 5\n",
    "batchsize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Define a covariance and mu for a gaussian\n",
    "mu  = np.array(rng.rand(dim) * 10, dtype=theano.config.floatX)\n",
    "cov = np.array(rng.rand(dim, dim), dtype=theano.config.floatX)\n",
    "cov = (cov + cov.T) / 2.\n",
    "cov[np.arange(dim), np.arange(dim)] = 1.0\n",
    "cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "# Define energy function for a multi-variate Gaussian\n",
    "def gaussian_energy(x):\n",
    "    return 0.5 * (tt.dot((x - mu), cov_inv) * (x - mu)).sum(axis=1)\n",
    "\n",
    "# Declared shared random variable for positions\n",
    "position = shared(rng.randn(batchsize, dim).astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize our sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = HMC(position, gaussian_energy,\n",
    "        initial_stepsize=1e-3, stepsize_max=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute some tuning samples, to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_me = [sampler.draw() for r in range(burnin)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now draw `n_samples` for inference. This returns a tensor of dimension `(n_samples, batchsize, dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.asarray([sampler.draw() for r in range(n_samples)])\n",
    "samples = samples.reshape(-1, *samples.shape[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estimated mean: ', samples.mean(axis=0))\n",
    "print('estimated covariance:\\n', np.cov(samples.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here are the true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('target mean:', mu)\n",
    "print('target cov:\\n', cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also peek at the tuned step size and acceptance rate. Compare this rate to what is targeted in Metropolis sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('final stepsize', sampler.stepsize.get_value())\n",
    "print('final acceptance_rate', sampler.avg_acceptance_rate.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the samples generated by our HMC sampler yield an\n",
    "empirical mean and covariance matrix, which are very close to the true\n",
    "underlying parameters. The adaptive algorithm also seemed to work well\n",
    "as the final acceptance rate is close to our target of `0.9`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The No U-turn Sampler\n",
    "\n",
    "Hamiltonian Monte Carlo is a powerful and efficient MCMC sampler when set up appropriately. However, this typically requires careful tuning of the sampler parameters, such as tree depth, leapfrog step size and target acceptance rate. Fortunately, the NUTS algorithm takes care of some of this for us. Nevertheless, tuning must be carefully monitored for failures that frequently arise. This is particularly the case when fitting challenging models, such as those with high curvature or heavy tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of NUTS Sampling\n",
    "\n",
    "NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution. True to its name, it stops automatically when it starts to double back and retrace its steps.\n",
    "\n",
    "The algorithm employs **binary doubling**, which takes leapfrog steps in a randomly-chosen direction with respect to the initial gradient. That is, one step is taken in one direction, two in the next direction, then four, eight, etc. The result is a balanced, binary tree with nodes comprised of Hamiltonian states. \n",
    "\n",
    "![](images/binary_doubling.png)\n",
    "\n",
    "The doubling process builds a balanced binary tree whose leaf nodes correspond to\n",
    "position-momentum states. The number of doubling events is referred to as the **tree depth** of the simulation.\n",
    "\n",
    "Doubling is halted when the subtrajectory from the leftmost to the rightmost nodes of any balanced subtree of the overall binary tree starts to double back on itself\n",
    "\n",
    "![](images/uturn.png)\n",
    "\n",
    "The important consequence of this approach is that it results in path lengths that are neither too short or too long, but rather, can vary from sample to sample depending on the geometry. \n",
    "\n",
    "To ensure detailed balance, a slice variable is sampled from:\n",
    "\n",
    "$$ u \\sim \\text{Uniform}(0, \\exp[L(\\theta) - 0.5 r \\cdot r])$$\n",
    "\n",
    "where $r$ is the initial momentum vector. The next sample is then chosen uniformly from the points in the remaining balanced tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS in PyMC3\n",
    "\n",
    "Though we have seen how HMC is implemented in Python, we won't concern ourselves with its modified NUTS implementation. Instead, we will use PyMC3 to fit our models from here on out, which has a well-tested and documented version of NUTS that is used as the default sampling method for models consisting of continuous latent variables.\n",
    "\n",
    "### Example: Estimating IQ impairment from blood phenylalanine for phenylketonuria patients\n",
    "\n",
    "![pku screening](images/pku_newborn.jpg)\n",
    "\n",
    "Phenylketonuria (PKU) is a rare genetic disorder that results in a reduced capacity for individuals to metabolize phenylalanine (Phe), resulting in a buildup of the amino acid in the blood stream. The inability to control blood Phe levels is known to lead to severe cognitive impairment in children. The association of blood phenylalanine levels with IQ was meta-analyzed using a hierarchical mixed-effects model ([Fonnesbeck et al., 2012](https://pubmed.ncbi.nlm.nih.gov/23197105/)). The model below is a simplified version of this meta-analysis which we will use to demonstrate the use of NUTS to esitmate models in PyMC3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_data = pd.read_csv('../data/pku_data.csv')\n",
    "pku_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_papers = set(pku_data['Paper ID'])\n",
    "paper_map = {p:i for i,p in enumerate(unique_papers)}\n",
    "paper_id = pku_data['Paper ID'].replace(paper_map)\n",
    "phe_std = ((pku_data.Phe - pku_data.Phe.mean()) / pku_data.Phe.std()).values\n",
    "iq = pku_data['IQ'].values\n",
    "concurrent_measurement = pku_data['Concurrent'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        μ_int = pm.Normal('μ_int', mu=100, sigma=1e3)\n",
    "        σ_int = pm.HalfCauchy('σ_int', beta=5)\n",
    "        β_0 = pm.Normal('β_0', μ_int, sigma=σ_int, shape=len(unique_papers))\n",
    "        β_1 = pm.Normal('β_1', mu=0, sigma=1e3)\n",
    "        β_2 = pm.Normal('β_2', mu=0, sigma=1e3)\n",
    "\n",
    "        μ_iq = β_0[paper_id] + β_1*phe_std + β_2*concurrent_measurement\n",
    "\n",
    "        σ_iq = pm.HalfCauchy('σ_iq', beta=1)\n",
    "        iq_like = pm.Normal('iq_like', mu=μ_iq, sigma=σ_iq, observed=iq)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_model = generate_model()\n",
    "\n",
    "pm.model_to_graphviz(pku_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "A key aspect of using gradient-based MCMC is the initialization of the algorithm. The performance of HMC is generally very sensitive to the **mass matrix** (somethines called the *metric*) that is used to specify the kinetic energy function. Changes in its parameterization will generally affect the efficiency of sampling, as the step size parameter has to be decreased to maintain precision in the simulated trajectories.\n",
    "\n",
    "Essentially, the optimal mass matrix will be equal to the inverse covariance of the posterior. So, you can use tuning samples to help estimate the sample covariance, and use this estimated matrix as the inverse mass matrix for sampling. The simplest approach is to estimate the diagonal elements of the mass matrix, and use the resulting diagonal matrix for sampling.\n",
    "\n",
    "PyMC3 comes with a suite of methods for initialization:\n",
    "\n",
    "* `adapt_diag`: Start with a identity mass matrix and then adapt a diagonal based on the\n",
    "          variance of the tuning samples. All chains use the test value (usually the prior mean)\n",
    "          as starting point.\n",
    "* `jitter+adapt_diag`: Same as `adapt_diag`, but add uniform jitter in \\[-1, 1\\] to the\n",
    "          starting point in each chain.\n",
    "* `advi+adapt_diag`: Run ADVI and then adapt the resulting diagonal mass matrix based on the\n",
    "          sample variance of the tuning samples.\n",
    "* `advi+adapt_diag_grad`: Run ADVI and then adapt the resulting diagonal mass matrix based\n",
    "          on the variance of the gradients during tuning. \n",
    "* `map`: Use the MAP as starting point. This is discouraged.\n",
    "* `advi`: Run ADVI to estimate posterior mean and diagonal mass matrix.\n",
    "* `advi_map`: Initialize ADVI with MAP and use MAP as starting point.\n",
    "* `adapt_full`: Adapt a dense mass matrix using the sample covariances\n",
    "\n",
    "The default method is `jitter+adapt_diag`, as it seems to perform the best across a wide range of problems.\n",
    "\n",
    "Despite its availability, `map` initialization is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_model = generate_model()\n",
    "\n",
    "with pku_model:\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, init='map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_model = generate_model()\n",
    "\n",
    "with pku_model:\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, init='adapt_diag', random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['β_1', 'β_2']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=['β_0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_model = generate_model()\n",
    "\n",
    "with pku_model:\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, step=pm.HamiltonianMC(), random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['β_1', 'β_2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_model = generate_model()\n",
    "\n",
    "with pku_model:\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, step=pm.Metropolis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['β_1', 'β_2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS Hyperparameters\n",
    "\n",
    "In addition to mass matrix adaptation, users can change the default values for the key hyperparameters in NUTS sampling. Specifically, these include:\n",
    "\n",
    "* `target_accept` : The step size is tuned such that we\n",
    "  approximate this acceptance rate. Higher values like 0.9 or 0.95 often\n",
    "  work better for problematic posteriors\n",
    "* `max_treedepth` : The maximum depth of the trajectory tree, which defaults to 10.\n",
    "* `step_scale` : The initial guess for the step size scaled down by, according to $(1/n)^{1/4}$\n",
    "\n",
    "Often, you will receive warning messages that include recommendations for changing the default values of these parameters when there are problems with sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## References\n",
    "\n",
    "1. Team TTD, Al-Rfou R, Alain G, et al. (2016) [Theano: A Python framework for fast computation of mathematical expressions](https://arxiv.org/abs/1605.02688). arXiv.org.\n",
    "2. Neal, R. M. (2010) [MCMC using Hamiltonian dynamics](http://www.mcmchandbook.net/HandbookChapter5.pdf), in the Handbook of Markov Chain Monte Carlo, S. Brooks, A. Gelman, G. L. Jones, and X.-L. Meng (editors), Chapman & Hall / CRC Press, pp. 113-162.\n",
    "3. Betancourt, M. (2017). [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434). arXiv.org.\n",
    "4. Carroll, C. (2019) [Hamiltonian Monte Carlo from Scratch](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
